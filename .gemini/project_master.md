MLOps/LLMOps Pet-Проект
**Я Senior DevOps-инженер. Моя цель — освоить полный цикл LLMOps через создание Self-Hosted Code Intelligence Assistant (CIA): AI-системы, которая отвечает на вопросы по внутренней кодовой базе (например, «Как реализована авторизация в подсистеме процессинг?»), используя RAG и (опционально) fine-tuned small LLM.**

#### Контекст (Мои знания и Требования):
- **DevOps**: Глубокие знания Kubernetes, Terraform, Docker, GitLab CI, Ingress, Helm.
- **MLOps (База)**: Использую MLflow Tracking Server (локально), DVC для версионирования данных, знаком с концепцией drift.
- **Обучение**: Все ML/LLM-концепции (эмбеддинги, LoRA, RAG metrics, дрейф, AST-парсинг, инструктаж) должны объясняться **через аналогии из DevOps** (например: «LoRA — это как Helm-чарт поверх базового образа»).
- **Архитектура (Локальная)**: Минимизация затрат и обеспечение безопасности.

#### Договоренности по Развертыванию:
| Расположение | Компоненты | Обоснование |
|---|---|---|
| **Локальный гипервизор с K8s на vm (CPU + GPU)** | **Полный стек**: Kubeflow (KFP, KServe), MLflow, Qdrant, Langfuse, FastAPI-сервер | **Единая среда**: Все компоненты, включая обучение и инференс, работают локально. |

- **Принцип "Python-First SDK"**: Везде, где это возможно, мы используем официальные Python SDK (KFP, Kubeflow Trainer, KServe) для декларативного определения и управления ресурсами (пайплайны, обучение, деплой), минимизируя прямое написание и поддержку YAML-манифестов.

---

### Подробный План Действий (6 Этапов)

#### **Проект: Self-Hosted Code Intelligence Assistant (CIA)**
*Цель: Отвечать на вопросы по коду с объяснением + ссылками на файлы, без утечки данных.*

---

#### **Этап 1: Подготовка локальной инфраструктуры и данных**
- **1.1: Подготовка GPU-ноды в K8s**: Добавить в локальный Kubernetes-кластер worker-ноду с проброшенной видеокартой. Установить на нее драйверы NVIDIA.
- **1.2: Установка и настройка NVIDIA Device Plugin**: Развернуть в кластере `nvidia-device-plugin` и сконфигурировать его для работы в режиме **Time-Slicing**. Это позволит "разделить" одну физическую GPU на несколько виртуальных для тестирования скейлинга подов.
- **1.3: Развертывание стека MLOps**:
  - **1.3.1: Развертывание Kubeflow**: Установить платформу Kubeflow, включая ключевые компоненты KFP (Pipelines) и KServe.
  - **1.3.2: Развертывание Qdrant**: Развернуть векторную базу данных Qdrant.
  - **1.3.3: Развертывание Langfuse**: Развернуть сервер для мониторинга и трейсинга LLM-взаимодействий Langfuse.
- **1.4: Код как датасет**: Склонировать 2–3 внутренних репозитория (или open-source аналоги, например, `langchain`, `fastapi`).
- **1.5: DVC-версионирование**: Зафиксировать состояние репозиториев через DVC (чтобы RAG был воспроизводим).

---

#### **Этап 2: RAG-пайплайн на LangChain**
- **2.1: Загрузка и парсинг**: Использовать `LangChain` для загрузки репозиториев. Применить `RecursiveCharacterTextSplitter` со знанием синтаксиса (`from_language`), который под капотом использует **`tree-sitter`** для семантически корректного разбиения кода на функции и классы.
- **2.2: Метаданные**: С помощью `LangChain` обогатить каждый чанк метаданными: `repo`, `file_path`, `line_start`, `language`.
- **2.3: Эмбеддинги**: Генерировать векторы с помощью **`stella_en_1.5B_v5`** — оптимальный выбор по качеству и эффективности (1.5B). Как альтернативу для максимального качества можно рассмотреть **`SFR-Embedding-2_R`**.
- **2.4: Индексация в Qdrant**: Написать KFP-компонент `build_code_index`, который, используя `LangChain`'s Qdrant integration, индексирует код в локальную векторную базу.

---

#### **Этап 3: Fine-Tuning маленькой модели (Code Llama 3.1 8B или DeepSeek Coder V2)**
- **3.1: Цель fine-tuning**: Научить модель:
  - Понимать внутренние термины,
  - Всегда ссылаться на источник (`file:line`),
  - Не галлюцинировать.
- **3.2: Модели-кандидаты**: Выбрать одну из state-of-the-art моделей для кода, например, **`Meta Code Llama 3.1 8B`** или **`DeepSeek Coder V2`**.
- **3.3: Датасет для SFT**: Сгенерировать 200–500 пар (вопрос → ответ с кодом и ссылкой). Можно использовать GPT-4 для синтеза, но хранить только локально.
- **3.4: QLoRA-обучение с помощью Kubeflow Trainer v2**: Реализовать обучение, разделив его на две части согласно новой роле-ориентированной модели:
    - **`TrainingRuntime` (DevOps-задача)**: В Python-коде определить среду выполнения: базовый образ, зависимости (`unsloth`, `transformers`), и требования к ресурсам (запросить **все** виртуальные ресурсы GPU, например 5, чтобы монополизировать физическую карту).
    - **`TrainJob` (ML-задача)**: Обернуть логику обучения на `unsloth` в Python-функцию. Эта функция будет ссылаться на `TrainingRuntime` и запускаться как `TrainJob`, полностью абстрагируясь от нижележащей инфраструктуры Kubernetes.
- **3.5: MLflow-логирование**: Логировать гиперпараметры и LoRA-адаптер. Оценка качества будет производиться на следующем этапе.

---

#### **Этап 4: Оркестрация (KFP) и Автопродвижение**
- **4.1: KFP-пайплайн**: Добавить явный шаг регистрации модели:
  - `index_code` → `fine_tune_llm` → `evaluate_model (с использованием DeepEval)` → **`register_model`**
  - **`register_model`**: Этот KFP-компонент будет брать LoRA-адаптер (артефакт KFP) и, если метрики из `evaluate_model` хорошие, регистрировать его в **MLflow Model Registry**, присваивая версию.
- **4.2: Автопродвижение в MLflow**: Если новая модель лучше по ключевым метрикам **DeepEval** (Faithfulness, Answer Relevancy) — ставим тег `Production`.
- **4.3: Интеграция с RAG**: При инференсе подгружаем **только Production-адаптер**.

---

#### **Этап 5: Инференс, скейлинг и HA (KServe + vLLM)**
- **5.1: vLLM-образ**: Собираем с поддержкой LoRA и подключением к MLflow.
- **5.2: RAG-сервер (FastAPI + LangChain)**: Принимает вопрос → **использует LangChain RAG Chain** (который ищет в Qdrant, формирует промпт) → вызывает vLLM. Интегрировать с **Langfuse** для трейсинга.
- **5.3: Тест автомасштабирования инференса (KPA)**: Развернуть **маленькую pre-trained модель** (например, Gemma 2B) через KServe. В манифесте указать запрос на 1 виртуальный GPU-ресурс (`nvidia.com/gpu: 1`). Настроить KPA и провести нагрузочный тест, чтобы убедиться, что KServe создает новые поды на той же GPU-ноде.
- **5.4: «Боевой» деплой основной модели**: Развернуть нашу последнюю **основную, дообученную модель** `Production` из MLflow Registry через отдельный `InferenceService`. Этот сервис будет:
    1.  Запрашивать **все доступные виртуальные ресурсы GPU** (например, `nvidia.com/gpu: 5`), чтобы монополизировать всю карту для максимальной производительности.
    2.  Использовать **KServe Python SDK** для декларативного описания.
    3.  Активировать **Model Caching** и **Speculative Decoding** для максимальной скорости инференса.
- **5.5: Тест отказоустойчивости**: Убиваем ноду — проверяем восстановление.

---

#### **Этап 6: Мониторинг и Observability (DeepEval + Langfuse)**
- **6.1: Инфра-метрики (Prometheus + Grafana)**: Метрики vLLM (токены/сек), Qdrant (latency), GPU util.
- **6.2: Оценка в CI/CD (DeepEval)**: На этапе `evaluate_model` в KFP-пайплайне измеряем:
    - **Faithfulness**: Не галлюцинирует ли модель?
    - **Answer Relevancy**: Ответ релевантен вопросу?
    - **Context Precision/Recall**: Найден ли релевантный код?
- **6.3: Мониторинг в Production (Langfuse)**:
  - **Трейсинг**: Полная трассировка каждого запроса в реальном времени.
  - **Производительность и затраты**: Latency, токены на запрос, стоимость.
  - **Дрейф**: Анализ эмбеддингов запросов для отслеживания изменения тематики.
- **6.4: Логирование запросов**: Все данные в Langfuse используются для аудита и дообучения.
