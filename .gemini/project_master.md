MLOps/LLMOps Pet-Проект
**Я Senior DevOps-инженер. Моя цель — освоить полный цикл LLMOps через создание Self-Hosted Code Intelligence Assistant (CIA): AI-системы, которая отвечает на вопросы по внутренней кодовой базе (например, «Как реализована авторизация в подсистеме процессинг?»), используя RAG и (опционально) fine-tuned small LLM.**

#### Контекст (Мои знания и Требования):
- **DevOps**: Глубокие знания Kubernetes, Terraform, Docker, GitLab CI, Ingress, Helm.
- **MLOps (База)**: Использую MLflow Tracking Server (локально), DVC для версионирования данных, знаком с концепцией drift.
- **Обучение**: Все ML/LLM-концепции (эмбеддинги, LoRA, RAG metrics, дрейф, AST-парсинг, инструктаж) должны объясняться **через аналогии из DevOps** (например: «LoRA — это как Helm-чарт поверх базового образа»).
- **Архитектура (Локальная)**: Минимизация затрат и обеспечение безопасности.

#### Договоренности по Развертыванию:
| Расположение | Компоненты | Обоснование |
|---|---|---|
| **Локальный гипервизор с K8s на vm (CPU + GPU)** | **Полный стек**: deployKF (включая Kubeflow, KFP, KServe), MLflow, Qdrant, Langfuse, FastAPI-сервер | **Единая среда**: Все компоненты, включая обучение и инференс, работают локально. |

- **Принцип "Python-First SDK"**: Везде, где это возможно, мы используем официальные Python SDK (KFP, Kubeflow Trainer, KServe) для декларативного определения и управления ресурсами (пайплайны, обучение, деплой), минимизируя прямое написание и поддержку YAML-манифестов.

---

### Соглашения по Структуре Проекта

*Эталонная структура проекта для обеспечения масштабируемости, переиспользуемости и разделения ответственности.*

```
/Users/mtik/go/src/github.com/wbe7/codesense/
├── codesense.yaml           # - Главный файл конфигурации проекта
├── data/                      # - Все данные под управлением DVC
│   ├── raw/                   #   - Сырые данные (клонированные репозитории)
│   └── training/              #   - Датасеты для обучения
├── infra/                     # - Конфигурация инфраструктуры (IaC)
├── scripts/                   # - Вспомогательные, операционные скрипты
│   └── repositories-converge.py
├── src/                       # - Исходный код устанавливаемого пакета "codesense"
│   └── codesense/
│       ├── app/                 #   - FastAPI приложение (RAG-сервис)
│       ├── pipelines/           #   - Оркестрация пайплайнов и их KFP-компоненты
│       │   ├── indexing/
│       │   │   ├── run.py       #   - "Движок" пайплайна индексации
│       │   │   └── component.py #   - Описание KFP-компонента через Python SDK
│       │   └── finetuning/
│       │       ├── run.py       #   - "Движок" пайплайна обучения
│       │       └── component.py #   - Описание KFP-компонента через Python SDK
│       ├── processing/          #   - Логика обработки данных (парсеры, чанкеры, vector_store)
│       ├── ml/                  #   - Логика обучения моделей (training loops)
│       └── utils/               #   - Общие утилиты
├── .gemini/
├── .gitignore
├── requirements.txt
└── README.md
```

**Описание и Принципы:**

1.  **`codesense.yaml` (корень проекта):**
    - Единый источник правды о составе датасета (список URL репозиториев).
    - Может быть расширен другими глобальными конфигурациями проекта.

2.  **`data/`:**
    - **Управление через DVC:** Вся директория `data/` находится под контролем DVC. Мы отслеживаем родительские папки (`data/raw`, `data/training`), а не каждый файл по отдельности. Это упрощает управление версиями данных.
    - **`data/raw/`:** Содержит исходные, необработанные данные. В нашем случае — клоны репозиториев. Название `raw` выбрано как стандарт индустрии, что обеспечивает расширяемость (в будущем сюда могут попасть другие сырые данные, например, документация).
    - **`data/training/`:** Содержит датасеты, подготовленные для обучения моделей.

3.  **`src/codesense/`:**
    - **Python Пакет:** Весь основной код проекта оформлен как устанавливаемый пакет. Это позволяет создавать чистое окружение и управлять зависимостями, а также обеспечивает переиспользуемость кода между компонентами (`app`, `pipelines`).
    - **Разделение по доменам:** Внутренняя структура пакета разделена по функциональным областям (веб-сервис, пайплайны, обработка данных, обучение), что упрощает навигацию и поддержку.

4.  **`scripts/`:**
    - Здесь размещаются вспомогательные скрипты для выполнения вручную (например, `repositories-converge.py`).
    - **План на будущее:** Если возникнет потребность в автоматизации какого-либо скрипта (например, включение его в KFP-пайплайн), его логика будет отрефакторена и перенесена в соответствующий модуль внутри `src/codesense/`.

---

### Подробный План Действий (6 Этапов)

#### **Проект: Self-Hosted Code Intelligence Assistant (CIA)**
*Цель: Отвечать на вопросы по коду с объяснением + ссылками на файлы, без утечки данных.*

---

#### **Этап 1: Подготовка локальной инфраструктуры и данных**
- **1.1: Подготовка GPU-ноды в K8s и установка NVIDIA GPU Operator**: Добавить в локальный Kubernetes-кластер worker-ноду с проброшенной видеокартой. Установить и настроить NVIDIA GPU Operator, который автоматически развернет драйверы NVIDIA, контейнерный тулкит и `nvidia-device-plugin`. Сконфигурировать `nvidia-device-plugin` для работы в режиме **Time-Slicing** (для `gpu-small` нод) и **Exclusive** (для `gpu-standard` нод). Это позволит "разделить" одну физическую GPU на несколько виртуальных для тестирования скейлинга подов.
- **1.2: Развертывание стека MLOps**:
  - **1.2.1: Развертывание MLOps-платформы через deployKF**: Установить и настроить `deployKF`, который управляет развертыванием всего стека, включая Kubeflow (KFP, KServe), MLflow и другие компоненты. Использовать GitOps-подход для декларативного управления конфигурацией.
  - **1.2.2: Развертывание Qdrant**: Развернуть векторную базу данных Qdrant.
  - **1.2.3: Развертывание Langfuse**: Развернуть сервер для мониторинга и трейсинга LLM-взаимодействий Langfuse.
- **1.3: Настройка проекта и версионирование данных**:
    - **1.3.1: Настройка рабочего окружения**: Создать и активировать виртуальное окружение Python (`.venv`), установить зависимости из `requirements.txt`.
    - **1.3.2: Конфигурация DVC**: Настроить DVC для использования S3-совместимого удаленного хранилища, с сохранением учетных данных в локальном файле `.dvc/config.local`.
- **1.4: Автоматизированный сбор и версионирование кодовых баз**:
    - **1.4.1: Создание `codebases.yaml`**: Определить список Git-репозиториев для использования в качестве датасета в файле `codebases/codebases.yaml`.
    - **1.4.2: Реализация `repositories-converge.py`**: Создать Python-скрипт, который автоматически клонирует репозитории из `codebases.yaml`, удаляет их `.git` каталоги и версионирует их с помощью DVC.
    - **1.4.3: Первичная загрузка данных**: Выполнить скрипт `repositories-converge.py` для загрузки и версионирования исходных кодовых баз, а затем выполнить `dvc push` для их отправки в удаленное хранилище.

---

#### **Этап 2: RAG-пайплайн, Эталонное Качество и Сбор Метаданных**
*Цель: Создать работающий RAG, получить эталон качества от state-of-the-art LLM и заложить основу для генерации кликабельных ссылок на код.*

- **2.1: Загрузка и парсинг**: Использовать `LangChain` для загрузки репозиториев. Применить `RecursiveCharacterTextSplitter` со знанием синтаксиса (`from_language`), который под капотом использует **`tree-sitter`** для семантически корректного разбиения кода.
- **2.2: Обогащение Метаданными**:
    - Создать и использовать утилитарную функцию `parse_repository_info(url)`, которая принимает URL из `codebases.yaml` и возвращает структурированный объект, включающий `id`, `clone_url` и `web_url`.
    - При создании чанков обогащать их метаданными: `repo_id`, `file_path`, `line_start`, `language` и, самое главное, `web_url` (например, `https://github.com/google-gemini/gemini-cli`). Это основа для генерации точных ссылок.
- **2.3: Эмбеддинги**: Генерировать векторы с помощью **`Qwen/Qwen3-Embedding-0.6B`**.
- **2.4: Индексация в Qdrant**: Написать KFP-компонент `build_code_index`, который индексирует код и его метаданные в Qdrant.
- **2.5: Baseline RAG с Gemini Pro и Langfuse Prompt Management**:
    - Создать в UI Langfuse версионируемый промпт (например, с именем `rag-code-assistant`).
    - Создать FastAPI-сервер с RAG-цепочкой, которая использует Qdrant для поиска и **Gemini 2.5 Pro** (через API) для генерации ответа.
    - В коде FastAPI **не жестко кодировать промпт**, а получать его через Langfuse SDK (`langfuse.get_prompt("rag-code-assistant")`). Это отделяет логику от конфигурации (аналогично `ConfigMap` в Kubernetes).
    - В системном промпте указать модели требование генерировать ответ со ссылкой на источник в формате: `Source: {web_url}/blob/main/{file_path}#L{line_start}`.
- **2.6: Интеграция с Langfuse**: С самого начала интегрировать RAG-сервис с Langfuse для сбора трейсов и метрик качества (Faithfulness, Answer Relevancy). Это наш **"золотой стандарт"**, с которым будут сравниваться все последующие модели.

---

#### **Этап 3: Переход на Self-Hosted Модель и Fine-Tuning через Knowledge Distillation**
*Цель: Заменить внешнюю LLM на собственную маленькую модель, дообученную для понимания нашего кода, и измерить результат.*

- **3.1: Развертывание базовой маленькой модели**: Выбрать и развернуть через KServe + vLLM pre-trained модель для кода (например, `CodeGemma-2B`). Переключить RAG-сервис на нее и сравнить метрики в Langfuse с эталоном.
- **3.2: Knowledge Distillation (Создание датасета)**:
    - **3.2.1: Синтетический датасет**: Создать KFP-компонент, который использует **Gemini 2.5 Pro как "учителя"** для генерации высококачественных пар (вопрос -> идеальный ответ со ссылкой) на основе нашего кода.
    - **3.2.2: Курирование "Золотого" датасета**: Периодически анализировать трейсы в Langfuse, отбирать лучшие примеры реальных запросов и эталонных ответов, и сохранять их в **Langfuse Dataset**. 
    - **3.2.3: Комбинирование**: KFP-пайплайн будет запрашивать "золотой" датасет из Langfuse API и объединять его с синтетическим для создания финального обучающего набора.
- **3.3: QLoRA Fine-Tuning**: Реализовать шаг обучения в Kubeflow (`Training Operator` + `unsloth`), который дообучает нашу маленькую модель на комбинированном датасете.
- **3.4: Регистрация и Оценка**: Зарегистрировать LoRA-адаптер в MLflow Model Registry. Развернуть дообученную модель и провести финальное сравнение в Langfuse: `Эталон (Gemini Pro)` vs `Базовая small-LLM` vs `Fine-tuned small-LLM`.

---

#### **Этап 4: Автоматизация (CI/CT/CD для RAG)**
*Цель: Превратить ручные шаги в автоматизированный, непрерывный цикл обновления модели и векторной базы.*

- **4.1: Построение полного KFP-пайплайна**: Объединить все шаги в единый пайплайн: `Сбор кода` -> `Индексация` -> `Генерация датасета` -> `Fine-tuning` -> `Оценка` -> `Деплой`.
- **4.2: Zero-Downtime Indexing**: В шаге индексации реализовать логику с **Qdrant Aliases**: создать новую коллекцию -> проиндексировать -> атомарно переключить alias -> удалить старую.
- **4.3: Canary Deployment**: В шаге деплоя использовать **KServe Canary**, чтобы плавно перевести трафик на новую, дообученную модель после успешной оценки.
- **4.4 (Future Work): Настройка пороговых метрик**: На основе данных из Langfuse определить и реализовать автоматическое продвижение модели в "production" при достижении определенных порогов качества.
- **4.5 (Future Work): Детализация KFP-компонентов**: Детально проработать Docker-образы и зависимости для каждого шага пайплайна.

---

#### **Этап 5: Высокопроизводительный Инференс как Результат CI/CD**
*Цель: Обеспечить стабильную, быструю и масштабируемую работу финальной модели в рамках автоматизированного контура.*

- **5.1: Управление инференсом через KServe**: Пайплайн из Этапа 4 автоматически развертывает и обновляет `InferenceService`.
- **5.2: Оптимизация vLLM**: Убедиться, что инференс-сервер использует vLLM с поддержкой LoRA и последними оптимизациями (PagedAttention, Model Caching).
- **5.3: Автомасштабирование (KPA)**: Настроить и протестировать Knative Pod Autoscaler для `InferenceService`, чтобы он автоматически масштабировался под нагрузкой.

---

#### **Этап 6: Непрерывный Мониторинг и Observability**
*Цель: Поддерживать работоспособность и качество системы в долгосрочной перспективе.*

- **6.1: Инфра-метрики (Prometheus + Grafana)**: Настроить дашборды для мониторинга GPU, vLLM (токены/сек), Qdrant (latency).
- **6.2: Оценка качества в CI/CD (DeepEval)**: Интегрировать `DeepEval` в шаг `Оценка` KFP-пайплайна для формализованной проверки метрик (Faithfulness, Answer Relevancy, Context Precision/Recall).
- **6.3: Мониторинг в Production (Langfuse)**: Использовать Langfuse для непрерывного трейсинга, мониторинга производительности, затрат и **обнаружения дрейфа данных** в запросах пользователей.
- **6.4: Замыкание цикла**: Настроить процесс, при котором данные из Langfuse (неудачные ответы, новые темы запросов) регулярно анализируются и используются для создания новых задач на дообучение, запуская тем самым новый виток CI/CD пайплайна.
- **6.5: Автоматическая оценка (LLM-as-a-Judge)**: В дополнение к DeepEval, настроить в Langfuse автоматическую оценку ответов с помощью модели-судьи (Gemini Pro) по кастомным критериям (например, корректность формата ссылки, вежливость, отсутствие галлюцинаций).

